{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install neo4j langchain-experimental\n",
    "!pip install llama-index llama-index-llms-groq groq llama-index-embeddings-huggingface ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, ServiceContext, load_index_from_storage\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.groq import Groq\n",
    "import warnings\n",
    "import os\n",
    "from neo4j import GraphDatabase\n",
    "import spacy\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "neo4j_uri = \"ENTER URI\"\n",
    "neo4j_user = \"neo4j\"\n",
    "neo4j_password = \"ENTER INSTANCE PASSWORD\"\n",
    "driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"ENTER API KEY\"\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Graph Insights: {graph_insights}\n",
    "Question: {question}\n",
    "\n",
    "Answer the question and provide additional helpful information,\n",
    "based on the pieces of information and graph insights, if applicable. Be succinct.\n",
    "\n",
    "Responses should be properly formatted to be easily read.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"This directory contains multiple documents providing examples and solutions for various programming tasks.\"\n",
    "directory_path = \"ENTER PATH\"\n",
    "reader = SimpleDirectoryReader(input_dir=directory_path)\n",
    "documents = reader.load_data()\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_graph(documents, driver, nlp):\n",
    "    with driver.session() as session:\n",
    "        for doc in documents:\n",
    "            doc_text = doc.text\n",
    "            nlp_doc = nlp(doc_text)\n",
    "            concepts = [ent.text for ent in nlp_doc.ents if ent.label_ == \"ORG\" or ent.label_ == \"PRODUCT\"] # Adjust entity types as needed\n",
    "\n",
    "            for concept in concepts:\n",
    "                session.run(\"MERGE (:Concept {name: $concept})\", concept=concept)\n",
    "\n",
    "            for i, concept in enumerate(concepts):\n",
    "                if i + 1 < len(concepts):\n",
    "                    next_concept = concepts[i + 1]\n",
    "                    session.run(\n",
    "                        \"\"\"\n",
    "                        MATCH (c1:Concept {name: $concept}), (c2:Concept {name: $next_concept})\n",
    "                        MERGE (c1)-[:RELATED_TO]->(c2)\n",
    "                        \"\"\",\n",
    "                        concept=concept, next_concept=next_concept\n",
    "                    )\n",
    "\n",
    "populate_graph(documents, driver, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=200)\n",
    "nodes = text_splitter.get_nodes_from_documents(documents, show_progress=True)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "llm = Groq(model=\"llama3-70b-8192\", api_key=GROQ_API_KEY)\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm)\n",
    "vector_index = VectorStoreIndex.from_documents(documents, show_progress=True, service_context=service_context, node_parser=nodes)\n",
    "vector_index.storage_context.persist(persist_dir=\"./storage_mini\")\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./storage_mini\")\n",
    "index = load_index_from_storage(storage_context, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_box = widgets.Text(\n",
    "    value='Explain Python?',\n",
    "    placeholder='Type your question here',\n",
    "    description='Question:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "\n",
    "def get_graph_insights(question):\n",
    "  with driver.session() as session:\n",
    "    result = session.run(\n",
    "         \"\"\"\n",
    "            MATCH (c:Concept)\n",
    "            WHERE toLower(c.name) CONTAINS toLower($question)\n",
    "            OPTIONAL MATCH (c)-[r:RELATED_TO]->(other:Concept)\n",
    "            RETURN c.name AS concept, collect(other.name) AS related_concepts\n",
    "            \"\"\",\n",
    "         question=question\n",
    "         )\n",
    "    insights = []\n",
    "    for record in result:\n",
    "       insights.append(f\"Concept: {record['concept']}, Related Concepts: {', '.join(record['related_concepts'])}\")\n",
    "       return \"\\n\".join(insights) if insights else \"No relevant graph insights found.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_button_click(b):\n",
    "  with output_area:\n",
    "    output_area.clear_output()\n",
    "    question = input_box.value\n",
    "    graph_insights = get_graph_insights(question)\n",
    "    query_prompt = prompt_template.format(context=context, graph_insights=graph_insights, question=question)\n",
    "    resp = query_engine.query(query_prompt)\n",
    "    print(resp.response)\n",
    "\n",
    "button = widgets.Button(\n",
    "    description='Ask',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltip='Ask the question',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "display(input_box, button, output_area)\n",
    "query_engine = index.as_query_engine(service_context=service_context)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
